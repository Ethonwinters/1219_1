ğŸ•ğŸ”ğŸŸğŸŒ­ğŸ¿ğŸ§‚ğŸ¥“ğŸ¥šğŸ¥¯ğŸ¥¨ğŸ¥ğŸğŸ§ˆğŸ¥ğŸ§‡ğŸ³ğŸ¥–ğŸ§€ğŸ¥—ğŸ¥™ğŸ¥™ğŸ¥ªğŸ±ğŸ£ğŸ¥ ğŸ¦ªğŸœğŸ¥ŸğŸ¥©ğŸ›ğŸ ğŸšğŸ™ğŸ—ğŸ˜ğŸ–
import os
import shutil

# å®šä¹‰æºæ–‡ä»¶å¤¹è·¯å¾„
input_folder = r'C:\Users\wang.zhuo17\V1\test\input'
target_folder = r'C:\Users\wang.zhuo17\V1\test\target'
destination_folder = r'C:\Users\wang.zhuo17\V1\test'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥å‰ªåˆ‡å¹¶ç§»åŠ¨æ–‡ä»¶
def move_files(source_folder, destination_folder):
    # è·å–æºæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(source_folder):
        file_path = os.path.join(source_folder, filename)
        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œåˆ™å‰ªåˆ‡å¹¶ç§»åŠ¨åˆ°ç›®æ ‡æ–‡ä»¶å¤¹
        if os.path.isfile(file_path):
            shutil.move(file_path, os.path.join(destination_folder, filename))
            print(f'æ–‡ä»¶ {filename} å·²ç§»åŠ¨åˆ° {destination_folder}')

# ç§»åŠ¨ input å’Œ target æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
move_files(input_folder, destination_folder)
move_files(target_folder, destination_folder)

print("æ‰€æœ‰æ–‡ä»¶å·²æˆåŠŸç§»åŠ¨ï¼")
ğŸ•ğŸ”ğŸŸğŸŒ­ğŸ¿ğŸ§‚ğŸ¥“ğŸ¥šğŸ¥¯ğŸ¥¨ğŸ¥ğŸğŸ§ˆğŸ¥ğŸ§‡ğŸ³ğŸ¥–ğŸ§€ğŸ¥—ğŸ¥™ğŸ¥™ğŸ¥ªğŸ±ğŸ£ğŸ¥ ğŸ¦ªğŸœğŸ¥ŸğŸ¥©ğŸ›ğŸ ğŸšğŸ™ğŸ—ğŸ˜ğŸ–
import torch
from sklearn.metrics import r2_score

# å‡è®¾ä½ çš„å¼ é‡å¦‚ä¸‹ï¼š
output_tensor_reverse = torch.randn(1, 20, 20)  # çœŸå®å€¼
Predicted_output_reverse = torch.randn(1, 20, 20)  # é¢„æµ‹å€¼

# æ‰å¹³åŒ–å¼ é‡
y_true = output_tensor_reverse.view(-1).cpu().numpy()  # çœŸå®å€¼ï¼Œè½¬ä¸ºnumpyæ•°ç»„
y_pred = Predicted_output_reverse.view(-1).cpu().numpy()  # é¢„æµ‹å€¼ï¼Œè½¬ä¸ºnumpyæ•°ç»„

# è®¡ç®— RÂ²
r2 = r2_score(y_true, y_pred)
print(f"RÂ²: {r2}")
ğŸ•ğŸ”ğŸŸğŸŒ­ğŸ¿ğŸ§‚ğŸ¥“ğŸ¥šğŸ¥¯ğŸ¥¨ğŸ¥ğŸğŸ§ˆğŸ¥ğŸ§‡ğŸ³ğŸ¥–ğŸ§€ğŸ¥—ğŸ¥™ğŸ¥™ğŸ¥ªğŸ±ğŸ£ğŸ¥ ğŸ¦ªğŸœğŸ¥ŸğŸ¥©ğŸ›ğŸ ğŸšğŸ™ğŸ—ğŸ˜ğŸ–
# å‡è®¾ä½ çš„å¼ é‡å¦‚ä¸‹ï¼š
output_tensor_reverse = torch.randn(1, 20, 20)  # çœŸå®å€¼
Predicted_output_reverse = torch.randn(1, 20, 20)  # é¢„æµ‹å€¼

# æ‰å¹³åŒ–å¼ é‡
y_true = output_tensor_reverse.view(-1)  # çœŸå®å€¼
y_pred = Predicted_output_reverse.view(-1)  # é¢„æµ‹å€¼

# è®¡ç®—å‡å€¼
y_mean = y_true.mean()

# è®¡ç®—RÂ²
ss_total = torch.sum((y_true - y_mean) ** 2)  # æ€»å¹³æ–¹å’Œ
ss_residual = torch.sum((y_true - y_pred) ** 2)  # æ®‹å·®å¹³æ–¹å’Œ

r2 = 1 - (ss_residual / ss_total)  # RÂ²å…¬å¼
print(f"RÂ²: {r2.item()}")
ğŸ•ğŸ”ğŸŸğŸŒ­ğŸ¿ğŸ§‚ğŸ¥“ğŸ¥šğŸ¥¯ğŸ¥¨ğŸ¥ğŸğŸ§ˆğŸ¥ğŸ§‡ğŸ³ğŸ¥–ğŸ§€ğŸ¥—ğŸ¥™ğŸ¥™ğŸ¥ªğŸ±ğŸ£ğŸ¥ ğŸ¦ªğŸœğŸ¥ŸğŸ¥©ğŸ›ğŸ ğŸšğŸ™ğŸ—ğŸ˜ğŸ–

test_loader_iter = DataLoader(dataset=test_dataset_, batch_size=256, shuffle=False,drop_last=True)   çœ‹æ¯ä¸ªbatchçš„æ–‡ä»¶å

# Step 2: Load the best model state
best_model = torch.load('best_model.pth')

# Step 3: Load state dict into the model
net.load_state_dict(best_model)

# Step 4: Set the model to evaluation mode
net.eval()

# Step 5: Evaluate on the test set
test_loss = 0.0

with torch.no_grad():
    for X_test, y_test in test_loader_iter:
        outputs = net(X_test)
        test_loss += loss(outputs, y_test).item() * X_test.size(0)

# Calculate average test loss
test_loss = test_loss / len(test_loader_iter.dataset)

print(f'Test Loss: {test_loss:.4f}')  è¿™æ ·è¾“å‡ºçš„æ˜¯ç¬¬å‡ ä¸ªbatch



ğŸ•ğŸ”ğŸŸğŸŒ­ğŸ¿ğŸ§‚ğŸ¥“ğŸ¥šğŸ¥¯ğŸ¥¨ğŸ¥ğŸğŸ§ˆğŸ¥ğŸ§‡ğŸ³ğŸ¥–ğŸ§€ğŸ¥—ğŸ¥™ğŸ¥™ğŸ¥ªğŸ±ğŸ£ğŸ¥ ğŸ¦ªğŸœğŸ¥ŸğŸ¥©ğŸ›ğŸ ğŸšğŸ™ğŸ—ğŸ˜ğŸ–
best_model = torch.load('best_model_1_1.pth')  
net.load_state_dict(best_model)  
net.eval()  

# Step 2: åŠ è½½ input{i}.pt å’Œ target{i}.pt æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
test_folder = r'C:\Users\wang.zhuo17\ENGINE_normalized_data_shuffled\test'  # è¾“å…¥æ–‡ä»¶æ‰€åœ¨ç›®å½•
input_files = sorted([f for f in os.listdir(test_folder) if f.startswith('input') and f.endswith('.pt')])  # æŸ¥æ‰¾æ‰€æœ‰ input{i}.pt æ–‡ä»¶
target_files = sorted([f for f in os.listdir(test_folder) if f.startswith('output') and f.endswith('.pt')])  # æŸ¥æ‰¾æ‰€æœ‰ target{i}.pt æ–‡ä»¶

# Step 3: åªå¤„ç†å‰ 600 ä¸ªæ–‡ä»¶
input_files = input_files[:600]  # åªé€‰æ‹©å‰ 600 ä¸ª input æ–‡ä»¶
target_files = target_files[:600]  # åªé€‰æ‹©å‰ 600 ä¸ª target æ–‡ä»¶

# Step 4: åˆå§‹åŒ–ä¿å­˜ç»“æœçš„åˆ—è¡¨
A_values = []  # ç”¨äºå­˜å‚¨ A å€¼
predicted_outputs = []  # ç”¨äºå­˜å‚¨é¢„æµ‹ç»“æœ
true_outputs = []  # ç”¨äºå­˜å‚¨çœŸå®ç»“æœ

# Step 5: é€ä¸ªåŠ è½½ input å’Œ target æ–‡ä»¶ï¼Œè¿›è¡Œé¢„æµ‹å¹¶è®¡ç®— A
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
net.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š

for input_file, target_file in zip(input_files, target_files):
    # åŠ è½½è¾“å…¥å’Œç›®æ ‡å¼ é‡
    input_tensor = torch.load(os.path.join(test_folder, input_file)).to(device)
    output_tensor = torch.load(os.path.join(test_folder, target_file)).to(device)
    
    # è¿›è¡Œé¢„æµ‹
    with torch.no_grad():
        predicted_output = (net(input_tensor.unsqueeze(0) * target_std)) + target_mean  # é¢„æµ‹ç»“æœ
        output_tensor = output_tensor * target_std + target_mean  # è½¬æ¢ç›®æ ‡å¼ é‡åˆ°åŸå§‹å°ºåº¦
    
    # è®¡ç®— A å€¼
    A = (output_tensor - predicted_output) / output_tensor  # A è®¡ç®—å…¬å¼

    # å°†é¢„æµ‹ç»“æœã€çœŸå®ç»“æœå’Œ A å€¼è½¬å› CPU å¹¶è½¬ä¸º NumPy æ•°ç»„ (å»é™¤ batch ç»´åº¦)
    predicted_outputs.append(predicted_output.squeeze().cpu().numpy())  # å»æ‰ batch ç»´åº¦ï¼Œå¾—åˆ° (20, 20)
    true_outputs.append(output_tensor.squeeze().cpu().numpy())  # å»æ‰ batch ç»´åº¦ï¼Œå¾—åˆ° (20, 20)
    A_values.append(A.squeeze().cpu().numpy())  # è®¡ç®— A å€¼ï¼Œå¹¶å»æ‰ batch ç»´åº¦ï¼Œå¾—åˆ° (20, 20)

# Step 6: å°†ç»“æœè½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå½¢çŠ¶åº”ä¸º (600, 20, 20)
predicted_outputs = np.array(predicted_outputs)  # å½¢çŠ¶ä¸º (600, 20, 20)
true_outputs = np.array(true_outputs)  # å½¢çŠ¶ä¸º (600, 20, 20)
A_matrix = np.array(A_values)  # å½¢çŠ¶ä¸º (600, 20, 20)

# æ‰“å°è¾“å‡ºçš„å½¢çŠ¶
print(f"Predicted outputs shape: {predicted_outputs.shape}")  # æ‰“å°é¢„æµ‹è¾“å‡ºçš„å½¢çŠ¶
print(f"True outputs shape: {true_outputs.shape}")  # æ‰“å°çœŸå®è¾“å‡ºçš„å½¢çŠ¶
print(f"A matrix shape: {A_matrix.shape}")  # æ‰“å° A çš„å½¢çŠ¶

# Step 7: è¾“å‡ºæœ€åä¸€ä¸ªé¢„æµ‹ç»“æœå’ŒçœŸå®ç»“æœ
print(f"Last predicted_output: {predicted_outputs[-1]}")  # æ‰“å°æœ€åä¸€ä¸ªé¢„æµ‹ç»“æœ
print(f"Last true output_tensor: {true_outputs[-1]}")  # æ‰“å°æœ€åä¸€ä¸ªçœŸå®ç»“æœ
print(f"Last A: {A_matrix[-1]}")  # æ‰“å°æœ€åä¸€ä¸ª A å€¼
